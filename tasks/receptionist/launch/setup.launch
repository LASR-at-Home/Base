<launch>

    <!--     PARAMS     -->
    <arg name="config" default="lab"/>

    <!--     INTERACTION     -->
    <arg name="whisper_device_param" default="9" />
    <node pkg="lasr_speech_recognition_whisper" type="transcribe_microphone_server" name="transcribe_speech" output="screen" args="--mic_device $(arg whisper_device_param)"/>

    <!-- LLM -->
    <node pkg="lasr_llm" type="llm.py" name="llm_server" output="screen"/>

    <!--     STATIC POINTS     -->
    <rosparam command="load" file="$(find receptionist)/config/$(arg config).yaml" ns="receptionist" />

    <!--     MOTIONS     -->
    <rosparam command="load" file="$(find lasr_skills)/config/motions.yaml"/>
    <rosparam command="load" file="$(find receptionist)/config/motions.yaml"/>

    <!--      PERCEPTION     -->
    <node pkg="lasr_vision_clip" name="clip_service" type="vqa" output="screen"/>
    <node pkg="lasr_vision_reid" type="service.py" name="reid_service" output="screen"/>
    <node pkg ="lasr_vision_eye_tracker" type="eye_tracker_server.py" name="eye_tracker_service" output="screen"/>
    <node name="yolo" pkg="lasr_vision_yolo" type="service.py" output="screen"/>
    <node pkg="lasr_sentence_embedding" type="sentence_embedding_server.py" name="sentence_embedding_service" output="screen"/>


</launch>
